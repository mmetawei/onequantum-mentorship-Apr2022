{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18e2b9d-e78b-4c01-aa8c-706b89948e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "tokenizers>=0.11.1,!=0.11.3,<0.13 is required for a normal functioning of this module, but found tokenizers==0.13.2.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\lambeq\\__init__.py:98\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2021-2022 Cambridge Quantum Computing Ltd.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version_info__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantumTrainer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     96\u001b[0m ]\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ansatz, core, pregroups, rewrite,\n\u001b[0;32m     99\u001b[0m                     text2diagram, tokeniser, training)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mansatz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BaseAnsatz, CircuitAnsatz, IQPAnsatz, MPSAnsatz,\n\u001b[0;32m    101\u001b[0m                            SpiderAnsatz, Symbol, TensorAnsatz)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VerbosityLevel\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\lambeq\\text2diagram\\__init__.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext2diagram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Reader\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext2diagram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mccg_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CCGParser\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext2diagram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbobcat_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BobcatParseError, BobcatParser\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext2diagram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mccgbank_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CCGBankParseError, CCGBankParser\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext2diagram\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdepccg_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepCCGParseError, DepCCGParser\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\lambeq\\text2diagram\\bobcat_parser.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TqdmWarning\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbobcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BertForChartClassification, Category,\n\u001b[0;32m     44\u001b[0m                            ChartParser, Grammar, ParseTree,\n\u001b[0;32m     45\u001b[0m                            Sentence, Supertag, Tagger)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VerbosityLevel\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\transformers\\__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     33\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     logging,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\transformers\\dependency_versions_check.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tokenizers_available():\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\transformers\\utils\\versions.py:122\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[1;34m(requirement)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: pip install transformers -U or pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\transformers\\utils\\versions.py:116\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 116\u001b[0m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\womanium\\lib\\site-packages\\transformers\\utils\\versions.py:49\u001b[0m, in \u001b[0;36m_compare_versions\u001b[1;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant_ver is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: tokenizers>=0.11.1,!=0.11.3,<0.13 is required for a normal functioning of this module, but found tokenizers==0.13.2.\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from lambeq import *\n",
    "from helper import *\n",
    "from lambeq import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f3a6e-1bd4-4d91-af9a-359fe61bac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 120\n",
    "SEED = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a168cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: lambeq\n",
      "Version: 0.2.6\n",
      "Summary: A QNLP toolkit\n",
      "Home-page: https://cqcl.github.io/lambeq\n",
      "Author: Cambridge Quantum QNLP team\n",
      "Author-email: lambeq-support@cambridgequantum.com\n",
      "License: Apache-2.0\n",
      "Location: c:\\users\\user\\.conda\\envs\\womanium\\lib\\site-packages\n",
      "Requires: discopy, pytket, pyyaml, spacy, tensornetwork, torch, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\user\\.conda\\envs\\womanium\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip show lambeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0b972d-3aa2-4dec-9258-9e3c026720b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMC1.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"MC1.txt\", header=None, sep=\", \", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cc1c6-ea6d-4ae4-9b03-17c6f97cb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"s1\", \"s2\", \"label\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de592c05-d187-4843-b63a-f8c3c8683aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "df_train_val, df_test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5)\n",
    "rskf_splits = list(rskf.split(df_train_val[[\"s1\", \"s2\"]], y=df_train_val[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171b009-fd63-4176-b5ac-f9bbe1b73ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum.circuit import Sim14ansatz\n",
    "from discopy.quantum.gates import CX, Controlled, X\n",
    "from discopy.tensor import Tensor\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850622ee-df39-45c4-bca8-dc04e7ca3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import  AtomicType\n",
    "#from lambeq.ansatz.circuit import _ArMapT\n",
    "from lambeq.ansatz.circuit import IQPAnsatz\n",
    "# Atomic types\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "\n",
    "ansatz = IQPAnsatz({N: 3, S: 1}, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958cb1c-74d7-4706-83a1-f82db4779d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = rskf_splits[0]\n",
    "df_train, df_val = df_train_val.iloc[train_idx], df_train_val.iloc[val_idx]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050b6c8-46e0-4d7a-aea6-2987cecb8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[df_train[\"label\"] == 0].shape)\n",
    "print(df_train[df_train[\"label\"] == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96da9f-ffa1-4d90-b0ae-98cee25366d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')\n",
    "preprocess_df(df_train, ansatz)\n",
    "preprocess_df(df_val, ansatz)\n",
    "preprocess_df(df_test, ansatz)\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106e316-b4e4-42b2-b2f7-197321424890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_circuits = list(df_train[[\"s1_circuit\", \"s2_circuit\"]].values)\n",
    "val_circuits = list(df_val[[\"s1_circuit\", \"s2_circuit\"]].values)\n",
    "test_circuits = list(df_test[[\"s1_circuit\", \"s2_circuit\"]].values)\n",
    "print(train_circuits[0][0].draw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67d47-486d-4c68-8bb2-10f61a3b0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_circuits = train_circuits + val_circuits + test_circuits\n",
    "assert len(all_circuits) == len(df)\n",
    "train_diagrams = list(df_train[[\"s1_diagrams\", \"s2_diagrams\"]].values)\n",
    "val_diagrams = list(df_val[[\"s1_diagrams\", \"s2_diagrams\"]].values)\n",
    "test_diagrams = list(df_test[[\"s1_diagrams\", \"s2_diagrams\"]].values)\n",
    "all_diagrams = train_diagrams + val_diagrams + test_diagrams         \n",
    "np.array(all_circuits).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5547450-573a-49fa-a916-7c855227a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = list(df_train[\"label_v\"].values)\n",
    "val_labels = list(df_val[\"label_v\"].values)\n",
    "test_labels = list(df_test[\"label_v\"].values)\n",
    "assert len(train_labels) > 0\n",
    "assert(test_labels)\n",
    "assert(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cd214-b476-4dd3-bd07-449a3f9f9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import IBMQBackend\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from qiskit.providers.aer.noise import NoiseModel\n",
    "from qiskit import IBMQ\n",
    "IBMQ.save_account('3c7bb882c186256cc5e989cf02f639e9282a264ee520c8882c6fb1c9ea51be3ce3d407629b8fed7450de430b7ccc59d4f97afddbba9b72b9daaebb40559ef0fc')\n",
    "provider = IBMQ.load_account()\n",
    "backend = AerBackend(NoiseModel.from_backend(IBMQ.providers()[0].get_backend('ibmq_manila')))\n",
    "\n",
    "backend_config = {\n",
    "    \"backend\": backend,\n",
    "    \"compilation\": backend.default_compilation_pass(2),\n",
    "    \"shots\": 2**10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46157d56-ad9c-4021-a23f-4c0a77b52e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTketModel.from_diagrams(np.array(all_circuits).reshape(-1), backend_config=backend_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835278b-325e-4766-a9d2-c4e348e35ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=np.finfo('float').eps\n",
    "loss = lambda y_hat, y: (-np.nansum(y * np.log(y_hat + epsilon)) + epsilon ) / len(y)\n",
    "acc = lambda y_hat, y: np.nansum(np.round(y_hat) == y) / len(y)/ 2 if (len(y) > 0) else None\n",
    "eval_metrics = {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181371ad-eed4-4d90-aed8-872f395e37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=loss,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={\"a\": 0.05, \"c\": 0.06, \"A\": 0.01 * EPOCHS},\n",
    "    evaluate_functions=eval_metrics,\n",
    "    verbose=\"text\",\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d503ac9-1b22-41a7-b041-0187a6293eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = Dataset(val_circuits, val_labels, shuffle=False)\n",
    "if not train_labels:\n",
    "    raise Exception(\"Empty train labels list!\")\n",
    "if not val_labels:\n",
    "    raise Exception(\"Empty test labels list!\")\n",
    "print(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885ad16-c79e-4f2e-98a4-153a3dd5fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "trainer.fit(train_dataset, val_dataset, logging_step=12)\n",
    "\n",
    "print(\"Training Time:--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e891f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('accuracy_vals_train_IQPAnsatz_N3_l2_noise.npy', trainer.train_results['acc'])\n",
    "np.save('accuracy_vals_val_IQPAnsatz_N3_l2_noise.npy', trainer.val_results['acc'])\n",
    "np.save('cost_vals_train_IQPAnsatz_N3_l2_noise.npy', trainer.train_epoch_costs)\n",
    "np.save('cost_vals_val_IQPAnsatz_N3_l2_noise.npy', trainer.val_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb3bda-7eff-48ed-8bb7-955336558703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharey='row', figsize=(10, 6))\n",
    "\n",
    "ax_tl.set_title('Training set')\n",
    "ax_tr.set_title('Development set')\n",
    "ax_bl.set_xlabel('Epochs')\n",
    "ax_br.set_xlabel('Epochs')\n",
    "ax_bl.set_ylabel('Accuracy')\n",
    "ax_tl.set_ylabel('Loss')\n",
    "\n",
    "colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ax_tl.plot(trainer.train_epoch_costs, color=next(colours))\n",
    "\n",
    "ax_bl.plot(trainer.train_results['acc'], color=next(colours))\n",
    "\n",
    "ax_tr.plot(trainer.val_costs, color=next(colours))\n",
    "\n",
    "ax_br.plot(trainer.val_results['acc'], color=next(colours))\n",
    "\n",
    "\n",
    "# print test accuracy\n",
    "test_acc = acc(model(test_circuits), test_labels)\n",
    "print('Test accuracy:', test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a53607-039d-474c-90b9-2f601b503af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum.circuit import IQPansatz as IQP\n",
    "pprint = lambda c: print(str(c).replace(' >>', '\\n  >>'))\n",
    "pprint(IQP(2, [[0.1], [0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd8383-5b5c-4cca-a97f-9e7b5ac5896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Mapping\n",
    "from lambeq import CircuitAnsatz\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "from discopy.quantum.circuit import (Circuit, Discard, Functor, Id,\n",
    "                                     IQPansatz as IQP, qubit, Sim14ansatz)\n",
    "from discopy.quantum.gates import Bra, Ket, Rx, Rz\n",
    "from discopy.rigid import Box, Diagram, Ty\n",
    "\n",
    "from lambeq.ansatz import Symbol\n",
    "from lambeq.ansatz.circuit import _ArMapT\n",
    "\n",
    "\n",
    "class CustomAnsatz(CircuitAnsatz):\n",
    "    \"\"\"S YC Chen, et al's deep reinforcement learning ansatz.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ob_map: Mapping[Ty, int],\n",
    "                 n_layers: int,\n",
    "                 n_single_qubit_params: int = 3,\n",
    "                 discard: bool = False,\n",
    "                 special_cases: Optional[Callable[[_ArMapT], _ArMapT]] = None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ob_map : dict\n",
    "            A mapping from :py:class:`discopy.rigid.Ty` to the number of\n",
    "            qubits it uses in a circuit.\n",
    "        n_layers : int\n",
    "            The number of IQP layers used by the ansatz.\n",
    "        n_single_qubit_params : int, default: 3\n",
    "            The number of single qubit rotations used by the ansatz.\n",
    "        discard : bool, default: False\n",
    "            Discard open wires instead of post-selecting.\n",
    "        special_cases : callable, optional\n",
    "            A function that transforms an arrow map into one specifying\n",
    "            special cases that should not be converted by the Ansatz\n",
    "            class.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(ob_map=ob_map, n_layers=n_layers,\n",
    "                         n_single_qubit_params=n_single_qubit_params)\n",
    "\n",
    "        if special_cases is None:\n",
    "            special_cases = self._special_cases\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_single_qubit_params = n_single_qubit_params\n",
    "        self.discard = discard\n",
    "        self.functor = Functor(ob=self.ob_map,\n",
    "                               ar=special_cases(self._ar))\n",
    "\n",
    "    def _ar(self, box: Box) -> Circuit:\n",
    "        label = self._summarise_box(box)\n",
    "        dom, cod = self._ob(box.dom), self._ob(box.cod)\n",
    "\n",
    "        n_qubits = max(dom, cod)\n",
    "        n_layers = self.n_layers\n",
    "        n_1qubit_params = self.n_single_qubit_params\n",
    "\n",
    "        if n_qubits == 0:\n",
    "            circuit = Id()\n",
    "        elif n_qubits == 1:\n",
    "            syms = [Symbol(f'{label}_{i}') for i in range(n_1qubit_params)]\n",
    "            rots = [Rx, Rz]\n",
    "            circuit = Id(qubit)\n",
    "            for i, sym in enumerate(syms):\n",
    "                circuit >>= rots[i % 2](sym)\n",
    "        else:\n",
    "            # We have three rotations for each qubit corresponding to\n",
    "            # the Euler decomposition\n",
    "            n_params = n_layers * n_qubits * 4\n",
    "            syms = [Symbol(f'{label}_{i}') for i in range(n_params)]\n",
    "            params: np.ndarray[Any, np.dtype[Any]] = np.array(syms).reshape(\n",
    "                    (n_layers, n_qubits, 3))\n",
    "            circuit = Sim14ansatz(n_qubits, params)\n",
    "\n",
    "        if cod > dom:\n",
    "            circuit <<= Id(dom) @ Ket(*[0]*(cod - dom))\n",
    "        elif self.discard:\n",
    "            circuit >>= Id(cod) @ Discard(dom - cod)\n",
    "        else:\n",
    "            circuit >>= Id(cod) @ Bra(*[0]*(dom - cod))\n",
    "        return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74887c-8a96-4bd9-b3b5-43562177512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, remove_cups\n",
    "params = [[0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9],\n",
    "          [0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9],\n",
    "         [0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9]]\n",
    "sentence = \"John cooks delicious food\"\n",
    "parser = BobcatParser()\n",
    "diagram = parser.sentence2diagram(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95bd19-cdfd-4b18-92dc-c9236b831c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic types\n",
    "from lambeq import remove_cups, AtomicType\n",
    "import numpy as np\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "Sim14Ansatz = _sim_ansatz_factory(14)\n",
    "Sim15Ansatz = _sim_ansatz_factory(15)\n",
    "ansatz = Sim14Ansatz({N: 1, S: 1}, n_layers=1)\n",
    "discopy_Circ2 = ansatz(remove_cups(diagram))\n",
    "discopy_Circ2.draw(figsize=(20, 20), fontsize=12)\n",
    "discopy_circuit = Sim14ansatz(3, params)\n",
    "discopy_circuit.draw(figsize=(20, 20), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae61e62-01b2-4b8c-9f29-369cab512f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Callable, Mapping\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import os\n",
    "import random\n",
    "import socket\n",
    "import sys\n",
    "from typing import Any, Optional, Union\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "from discopy import Tensor\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lambeq.core.globals import VerbosityLevel\n",
    "from lambeq.training.checkpoint import Checkpoint\n",
    "from lambeq.training.dataset import Dataset\n",
    "from lambeq.training.model import Model\n",
    "\n",
    "\n",
    "def _import_tensorboard_writer() -> None:\n",
    "    global SummaryWriter\n",
    "    try:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "    except ImportError:  # pragma: no cover\n",
    "        raise ImportError('tensorboard not found. Please install it using '\n",
    "                          '`pip install tensorboard`.')\n",
    "\n",
    "\n",
    "_StrPathT = Union[str, 'os.PathLike[str]']\n",
    "\n",
    "\n",
    "class CustomQuantumTrainer(QuantumTrainer):\n",
    "\n",
    "\n",
    "    def validation_step(\n",
    "            self,\n",
    "            batch: tuple[list[Any], np.ndarray]) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Perform a validation step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : tuple of list and np.ndarray\n",
    "            Current batch.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of np.ndarray and float\n",
    "            The model predictions and the calculated loss.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        assert len(y) > 0\n",
    "        #assert y_hat != 0\n",
    "        print('Size of y from validation function', len(y))\n",
    "        print('log(y_hat) from validation function', np.log(y_hat))\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "        numerator_loss = -np.sum(y * np.log(y_hat + epsilon)) + epsilon\n",
    "        denom_loss = len(y)\n",
    "        print('calculated numerator loss from validation function', numerator_loss)\n",
    "        print('calculated denom loss from validation function', denom_loss)\n",
    "        print('calculated loss from validation function', loss)\n",
    "        return y_hat, loss\n",
    "    def fit(self,\n",
    "            train_dataset: Dataset,\n",
    "            val_dataset: Optional[Dataset] = None,\n",
    "            evaluation_step: int = 1,\n",
    "            logging_step: int = 1) -> None:\n",
    "        \"\"\"Fit the model on the training data and, optionally,\n",
    "        evaluate it on the validation data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_dataset : :py:class:`Dataset`\n",
    "            Dataset used for training.\n",
    "        val_dataset : :py:class:`Dataset`, optional\n",
    "            Validation dataset.\n",
    "        evaluation_step : int, default: 1\n",
    "            Sets the intervals at which the metrics are evaluated on the\n",
    "            validation dataset.\n",
    "        logging_step : int, default: 1\n",
    "            Sets the intervals at which the training statistics are\n",
    "            printed if `verbose = 'text'` (otherwise ignored).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.from_checkpoint:\n",
    "            self._load_extra_chkpoint_info(self.checkpoint)\n",
    "\n",
    "        def writer_helper(*args: Any) -> None:\n",
    "            if self.use_tensorboard:\n",
    "                self.writer.add_scalar(*args)\n",
    "            else:\n",
    "                print(*args)\n",
    "\n",
    "        # initialise progress bar\n",
    "        step = self.start_step\n",
    "        batches_per_epoch = ceil(len(train_dataset)/train_dataset.batch_size)\n",
    "        status_bar = tqdm(total=float('inf'),\n",
    "                          bar_format='{desc}',\n",
    "                          desc=self._generate_stat_report(),\n",
    "                          disable=(\n",
    "                                self.verbose != VerbosityLevel.PROGRESS.value),\n",
    "                          leave=True,\n",
    "                          position=0)\n",
    "\n",
    "        # start training loop\n",
    "        for epoch in trange(self.start_epoch,\n",
    "                            self.epochs,\n",
    "                            desc='Epoch',\n",
    "                            disable=(\n",
    "                                self.verbose != VerbosityLevel.PROGRESS.value),\n",
    "                            leave=False,\n",
    "                            position=1):\n",
    "            train_loss = 0.0\n",
    "            with Tensor.backend(self.backend):\n",
    "                for batch in tqdm(train_dataset,\n",
    "                                  desc='Batch',\n",
    "                                  total=batches_per_epoch,\n",
    "                                  disable=(self.verbose\n",
    "                                           != VerbosityLevel.PROGRESS.value),\n",
    "                                  leave=False,\n",
    "                                  position=2):\n",
    "                    step += 1\n",
    "                    x, y_label = batch\n",
    "                    y_hat, loss = self.training_step(batch)\n",
    "                    writer_helper('y_hat', y_hat)\n",
    "                    #if np.isnan(np.array(y_hat)).any():\n",
    "                     #   raise Exception(\"Empty evaluation values list after training step!\")\n",
    "                    #if np.isnan(np.array(loss)).any():\n",
    "                     #   raise Exception(\"Nan loss values list after training step!\")\n",
    "                    writer_helper('y_hat', y_hat)\n",
    "                    if np.isnan(np.array(y_hat)).any():\n",
    "                        raise Exception(\"Empty evaluation values list!\")\n",
    "                    if (self.evaluate_on_train\n",
    "                            and self.evaluate_functions is not None):\n",
    "                        for metr, func in self.evaluate_functions.items():\n",
    "                            res = func(y_hat, y_label)\n",
    "                            metric = self._train_results_epoch[metr]\n",
    "                            metric.append(len(x) * res)\n",
    "                    train_loss += len(batch[0]) * loss\n",
    "                    assert np.isnan(np.array(loss)).any() == False\n",
    "                    writer_helper('train/step_loss', loss, step)\n",
    "                    status_bar.set_description(\n",
    "                            self._generate_stat_report(\n",
    "                                train_loss=loss,\n",
    "                                val_loss=(self.val_costs[-1] if self.val_costs\n",
    "                                          else None)))\n",
    "            train_loss /= len(train_dataset)\n",
    "            assert len(train_dataset) > 0\n",
    "            print('y_label', y_label)\n",
    "            if np.isnan(np.array(y_label)).any():\n",
    "                raise Exception(\"Empty train label list!\")\n",
    "            if np.isnan(np.array(y_hat)).any():\n",
    "                raise Exception(\"Empty evaluation values list!\")\n",
    "            self.train_epoch_costs.append(train_loss)\n",
    "            writer_helper('train/epoch_loss', train_loss, epoch + 1)\n",
    "\n",
    "            # evaluate on train\n",
    "            if (self.evaluate_on_train\n",
    "                    and self.evaluate_functions is not None):\n",
    "                for name in self._train_results_epoch:\n",
    "                    self.train_results[name].append(\n",
    "                        sum(self._train_results_epoch[name])/len(train_dataset)\n",
    "                    )\n",
    "                    self._train_results_epoch[name] = []  # reset\n",
    "                    writer_helper(\n",
    "                        f'train/{name}', self.train_results[name][-1],\n",
    "                        epoch+1)\n",
    "                    if self.verbose == VerbosityLevel.PROGRESS.value:\n",
    "                        status_bar.set_description(\n",
    "                                self._generate_stat_report(\n",
    "                                    train_loss=train_loss,\n",
    "                                    val_loss=(self.val_costs[-1]\n",
    "                                              if self.val_costs else None)))\n",
    "\n",
    "            # evaluate metrics on validation data\n",
    "            if val_dataset is not None:\n",
    "                if epoch % evaluation_step == 0:\n",
    "                    val_loss = 0.0\n",
    "                    batches_per_validation = ceil(len(val_dataset)\n",
    "                                                  / val_dataset.batch_size)\n",
    "                    writer_helper('batches_per_validation', batches_per_validation, len(val_dataset), val_dataset.batch_size)\n",
    "                    with Tensor.backend(self.backend):\n",
    "                        disable_tqdm = (self.verbose\n",
    "                                        != VerbosityLevel.PROGRESS.value)\n",
    "                        for v_batch in tqdm(val_dataset,\n",
    "                                            desc='Validation batch',\n",
    "                                            total=batches_per_validation,\n",
    "                                            disable=disable_tqdm,\n",
    "                                            leave=False,\n",
    "                                            position=2):\n",
    "                            writer_helper(\"***\", v_batch)\n",
    "                            x_val, y_label_val = v_batch\n",
    "                            writer_helper(\"***\", x_val, y_label_val)\n",
    "                            y_hat_val, cur_loss = self.validation_step(v_batch)\n",
    "                            if np.isnan(np.array(y_hat)).any():\n",
    "                                raise Exception(\"Empty evaluation values list after validation step!\")\n",
    "                            writer_helper(\"***\", y_hat_val, cur_loss)\n",
    "                            val_loss += cur_loss * len(x_val)\n",
    "                            if self.evaluate_functions is not None:\n",
    "                                for metr, func in (\n",
    "                                        self.evaluate_functions.items()):\n",
    "                                    res = func(y_hat_val, y_label_val)\n",
    "                                    self._val_results_epoch[metr].append(\n",
    "                                        len(x_val)*res)\n",
    "                            status_bar.set_description(\n",
    "                                    self._generate_stat_report(\n",
    "                                        train_loss=train_loss,\n",
    "                                        val_loss=val_loss))\n",
    "                        assert len(val_dataset) > 0\n",
    "                        writer_helper('empty val dataset!', len(val_dataset))\n",
    "                        val_loss /= len(val_dataset)\n",
    "                        self.val_costs.append(val_loss)\n",
    "                        writer_helper('cost vals', self.val_costs)\n",
    "                        status_bar.set_description(\n",
    "                                self._generate_stat_report(\n",
    "                                    train_loss=train_loss,\n",
    "                                    val_loss=val_loss))\n",
    "                        if np.isnan(np.array(val_loss)).any():\n",
    "                            raise Exception(\"loss value is NaN!\")\n",
    "                        writer_helper('empty val dataset!', len(val_dataset))\n",
    "                        writer_helper('val/loss', val_loss, epoch+1)\n",
    "\n",
    "                    if self.evaluate_functions is not None:\n",
    "                        for name in self._val_results_epoch:\n",
    "                            assert len(val_dataset) > 0\n",
    "                            writer_helper('empty val dataset!', len(val_dataset))\n",
    "                            self.val_results[name].append(\n",
    "                                sum(self._val_results_epoch[name])\n",
    "                                / len(val_dataset))\n",
    "                            self._val_results_epoch[name] = []  # reset\n",
    "                            writer_helper(\n",
    "                                f'val/{name}', self.val_results[name][-1],\n",
    "                                epoch + 1)\n",
    "                            status_bar.set_description(\n",
    "                                    self._generate_stat_report(\n",
    "                                        train_loss=train_loss,\n",
    "                                        val_loss=val_loss))\n",
    "            # save checkpoint info\n",
    "            save_dict = {'epoch': epoch+1,\n",
    "                         'model_weights': self.model.weights,\n",
    "                         'model_symbols': self.model.symbols,\n",
    "                        'train_costs': self.train_costs,\n",
    "                         'train_epoch_costs': self.train_epoch_costs,\n",
    "                         'train_results': self.train_results,\n",
    "                         'val_costs': self.val_costs,\n",
    "                         'val_results': self.val_results,\n",
    "                         'random_state': random.getstate(),\n",
    "                         'step': step}\n",
    "            print(f\"save_dict: {save_dict}\")\n",
    "            self.save_checkpoint(save_dict, self.log_dir)\n",
    "            if self.verbose == VerbosityLevel.TEXT.value:\n",
    "                if epoch == 0 or (epoch+1) % logging_step == 0:\n",
    "                    space = (len(str(self.epochs))-len(str(epoch+1)) + 2) * ' '\n",
    "                    prefix = f'Epoch {epoch+1}:' + space\n",
    "                    print(prefix + self._generate_stat_report(\n",
    "                            train_loss=train_loss,\n",
    "                            val_loss=(self.val_costs[-1] if self.val_costs\n",
    "                                      else None)),\n",
    "                          file=sys.stderr)\n",
    "        status_bar.close()\n",
    "        if self.verbose == VerbosityLevel.TEXT.value:\n",
    "            print('\\nTraining completed!', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776f9f5-6fc5-46cf-b8b2-c691d83d1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = CustomQuantumTrainer(\n",
    "    model,\n",
    "    epochs=EPOCHS,\n",
    "    loss_function=loss,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.05, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22523d-800e-4bb9-94b4-b1905683d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.fit(train_dataset, val_dataset, logging_step=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432561ec-ea32-433f-9843-c8fef39eb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum import Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f584dda-bb59-460f-880b-0735b4f6896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_circuits).reshape(-1)\n",
    "all_circuits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2199504-eea9-4412-8d01-5b114cf94aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.circuit.display import render_circuit_jupyter\n",
    "render_circuit_jupyter(all_circuits[0][0].to_tk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a144b07-e07d-42cf-b0d3-5e44214316e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdified_diagram = model._get_lambda(train_circuits[0][0])\n",
    "print(len(val_circuits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77561f-de3a-4220-99be-827c22ff81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambdified_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a9be0-2163-4df0-b480-626f9192991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0ee14-1d4a-49f7-bd67-1e90e5507189",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = Circuit.eval(\n",
    "    *lambdified_diagram(*model.weights),\n",
    "    **model.backend_config,\n",
    "    seed=model._randint(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e57019-af88-4658-b457-c7e14a1be2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdified_diagram(*model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfa50b-4805-4c59-a262-c6c92fb504fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epsilon=np.finfo('float64').eps\n",
    "eps = 1e-50\n",
    "np.log(eps + epsilon)/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155cdbc-17cf-4d7d-88a5-5c7cd9e3d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1a0c0-07bb-4aab-9350-52438cea3e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
