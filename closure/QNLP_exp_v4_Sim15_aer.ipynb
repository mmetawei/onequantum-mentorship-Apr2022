{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18e2b9d-e78b-4c01-aa8c-706b89948e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\womanium\\lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from lambeq import QuantumTrainer, SPSAOptimizer\n",
    "from helper import *\n",
    "from lambeq import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0f3a6e-1bd4-4d91-af9a-359fe61bac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 120\n",
    "SEED = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0b972d-3aa2-4dec-9258-9e3c026720b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MC1.txt\", header=None, sep=\", \", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225cc1c6-ea6d-4ae4-9b03-17c6f97cb91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cook creates complicated dish</td>\n",
       "      <td>experienced chef prepares complicated dish</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>skilful programmer creates code</td>\n",
       "      <td>devoted hacker writes code</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skilful cook creates meal</td>\n",
       "      <td>devoted hacker creates complicated code</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hacker writes code</td>\n",
       "      <td>skilful hacker creates code</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>devoted hacker writes code</td>\n",
       "      <td>hacker writes complicated code</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                s1  \\\n",
       "0    cook creates complicated dish   \n",
       "1  skilful programmer creates code   \n",
       "2        skilful cook creates meal   \n",
       "3               hacker writes code   \n",
       "4       devoted hacker writes code   \n",
       "\n",
       "                                           s2  label  \n",
       "0  experienced chef prepares complicated dish      1  \n",
       "1                  devoted hacker writes code      1  \n",
       "2     devoted hacker creates complicated code      0  \n",
       "3                 skilful hacker creates code      1  \n",
       "4              hacker writes complicated code      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [\"s1\", \"s2\", \"label\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de592c05-d187-4843-b63a-f8c3c8683aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "df_train_val, df_test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5)\n",
    "rskf_splits = list(rskf.split(df_train_val[[\"s1\", \"s2\"]], y=df_train_val[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a171b009-fd63-4176-b5ac-f9bbe1b73ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from discopy.quantum.circuit import Sim14ansatz\n",
    "from discopy.quantum.gates import CX, Controlled, X\n",
    "from discopy.tensor import Tensor\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd58c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lambeq==0.2.7\n",
      "  Using cached lambeq-0.2.7-py3-none-any.whl (129 kB)\n",
      "Collecting spacy>=3.0\n",
      "  Using cached spacy-3.4.4-cp310-cp310-win_amd64.whl (11.9 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Collecting torch>=1.12.1\n",
      "  Using cached torch-1.13.1-cp310-cp310-win_amd64.whl (162.6 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting discopy>=0.4.3\n",
      "  Using cached discopy-0.4.3-py3-none-any.whl\n",
      "Collecting tensornetwork\n",
      "  Using cached tensornetwork-0.4.6-py3-none-any.whl (364 kB)\n",
      "Collecting pytket>=0.19.2\n",
      "  Using cached pytket-1.10.0-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "Collecting pillow>=6.2.1\n",
      "  Using cached Pillow-9.3.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Collecting numpy>=1.18.1\n",
      "  Using cached numpy-1.24.1-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Collecting networkx>=2.4\n",
      "  Using cached networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "Collecting matplotlib>=3.1.2\n",
      "  Using cached matplotlib-3.6.2-cp310-cp310-win_amd64.whl (7.2 MB)\n",
      "Collecting lark-parser~=0.7\n",
      "  Using cached lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
      "Collecting types-pkg-resources\n",
      "  Using cached types_pkg_resources-0.1.3-py2.py3-none-any.whl (4.8 kB)\n",
      "Collecting typing-extensions~=4.2\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting scipy<2.0,>=1.7.2\n",
      "  Using cached scipy-1.9.3-cp310-cp310-win_amd64.whl (40.1 MB)\n",
      "Collecting sympy~=1.6\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Collecting qwasm~=1.0\n",
      "  Using cached qwasm-1.0.1-py3-none-any.whl (15 kB)\n",
      "Collecting jinja2~=3.0\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting graphviz~=0.14\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Using cached thinc-8.1.6-cp310-cp310-win_amd64.whl (1.3 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.5-cp310-cp310-win_amd64.whl (479 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Using cached spacy_legacy-3.0.11-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Using cached pydantic-1.10.4-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-22.0-py3-none-any.whl (42 kB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Using cached typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.7.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Collecting opt-einsum>=2.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp310-cp310-win_amd64.whl (267 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.1-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Collecting python-dateutil>=2.7\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.6-cp310-cp310-win_amd64.whl (163 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: wasabi, types-pkg-resources, tokenizers, mpmath, lark-parser, cymem, urllib3, typing-extensions, sympy, spacy-loggers, spacy-legacy, smart-open, six, setuptools, regex, pyyaml, pyparsing, pillow, packaging, numpy, networkx, murmurhash, MarkupSafe, langcodes, kiwisolver, idna, graphviz, fonttools, filelock, cycler, colorama, charset-normalizer, certifi, catalogue, tqdm, torch, srsly, scipy, requests, qwasm, python-dateutil, pydantic, preshed, opt-einsum, jinja2, h5py, contourpy, click, blis, typer, tensornetwork, pytket, matplotlib, huggingface-hub, confection, transformers, thinc, pathy, discopy, spacy, lambeq\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 0.10.1\n",
      "    Uninstalling wasabi-0.10.1:\n",
      "      Successfully uninstalled wasabi-0.10.1\n",
      "  Attempting uninstall: types-pkg-resources\n",
      "    Found existing installation: types-pkg-resources 0.1.3\n",
      "    Uninstalling types-pkg-resources-0.1.3:\n",
      "      Successfully uninstalled types-pkg-resources-0.1.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.2.1\n",
      "    Uninstalling mpmath-1.2.1:\n",
      "      Successfully uninstalled mpmath-1.2.1\n",
      "  Attempting uninstall: lark-parser\n",
      "    Found existing installation: lark-parser 0.12.0\n",
      "    Uninstalling lark-parser-0.12.0:\n",
      "      Successfully uninstalled lark-parser-0.12.0\n",
      "  Attempting uninstall: cymem\n",
      "    Found existing installation: cymem 2.0.7\n",
      "    Uninstalling cymem-2.0.7:\n",
      "      Successfully uninstalled cymem-2.0.7\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: spacy-loggers\n",
      "    Found existing installation: spacy-loggers 1.0.4\n",
      "    Uninstalling spacy-loggers-1.0.4:\n",
      "      Successfully uninstalled spacy-loggers-1.0.4\n",
      "  Attempting uninstall: spacy-legacy\n",
      "    Found existing installation: spacy-legacy 3.0.11\n",
      "    Uninstalling spacy-legacy-3.0.11:\n",
      "      Successfully uninstalled spacy-legacy-3.0.11\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 6.3.0\n",
      "    Uninstalling smart-open-6.3.0:\n",
      "      Successfully uninstalled smart-open-6.3.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.6.3\n",
      "    Uninstalling setuptools-65.6.3:\n",
      "      Successfully uninstalled setuptools-65.6.3\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2022.10.31\n",
      "    Uninstalling regex-2022.10.31:\n",
      "      Successfully uninstalled regex-2022.10.31\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\User\\\\.conda\\\\envs\\\\womanium\\\\Lib\\\\site-packages\\\\~il\\\\_imaging.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --force-reinstall lambeq==0.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "850622ee-df39-45c4-bca8-dc04e7ca3897",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Sim15Ansatz' from 'lambeq.ansatz.circuit' (C:\\Users\\User\\.conda\\envs\\womanium\\lib\\site-packages\\lambeq\\ansatz\\circuit.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  AtomicType\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#from lambeq.ansatz.circuit import _ArMapT\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlambeq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mansatz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcircuit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sim15Ansatz\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Atomic types\u001b[39;00m\n\u001b[0;32m      5\u001b[0m N \u001b[38;5;241m=\u001b[39m AtomicType\u001b[38;5;241m.\u001b[39mNOUN\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Sim15Ansatz' from 'lambeq.ansatz.circuit' (C:\\Users\\User\\.conda\\envs\\womanium\\lib\\site-packages\\lambeq\\ansatz\\circuit.py)"
     ]
    }
   ],
   "source": [
    "from lambeq import  AtomicType\n",
    "#from lambeq.ansatz.circuit import _ArMapT\n",
    "from lambeq.ansatz.circuit import Sim15Ansatz\n",
    "# Atomic types\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "\n",
    "ansatz = Sim15Ansatz({N: 3, S: 1}, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958cb1c-74d7-4706-83a1-f82db4779d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = rskf_splits[0]\n",
    "df_train, df_val = df_train_val.iloc[train_idx], df_train_val.iloc[val_idx]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050b6c8-46e0-4d7a-aea6-2987cecb8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[df_train[\"label\"] == 0].shape)\n",
    "print(df_train[df_train[\"label\"] == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96da9f-ffa1-4d90-b0ae-98cee25366d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = BobcatParser(verbose='text')\n",
    "preprocess_df(df_train, ansatz)\n",
    "preprocess_df(df_val, ansatz)\n",
    "preprocess_df(df_test, ansatz)\n",
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106e316-b4e4-42b2-b2f7-197321424890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_circuits = list(df_train[[\"s1_circuit\", \"s2_circuit\"]].values)\n",
    "val_circuits = list(df_val[[\"s1_circuit\", \"s2_circuit\"]].values)\n",
    "test_circuits = list(df_test[[\"s1_circuit\", \"s2_circuit\"]].values)\n",
    "print(train_circuits[0][0].draw())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67d47-486d-4c68-8bb2-10f61a3b0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_circuits = train_circuits + val_circuits + test_circuits\n",
    "assert len(all_circuits) == len(df)\n",
    "train_diagrams = list(df_train[[\"s1_diagrams\", \"s2_diagrams\"]].values)\n",
    "val_diagrams = list(df_val[[\"s1_diagrams\", \"s2_diagrams\"]].values)\n",
    "test_diagrams = list(df_test[[\"s1_diagrams\", \"s2_diagrams\"]].values)\n",
    "all_diagrams = train_diagrams + val_diagrams + test_diagrams         \n",
    "np.array(all_circuits).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5547450-573a-49fa-a916-7c855227a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = list(df_train[\"label_v\"].values)\n",
    "val_labels = list(df_val[\"label_v\"].values)\n",
    "test_labels = list(df_test[\"label_v\"].values)\n",
    "assert len(train_labels) > 0\n",
    "assert(test_labels)\n",
    "assert(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cd214-b476-4dd3-bd07-449a3f9f9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.extensions.qiskit import IBMQBackend\n",
    "from pytket.extensions.qiskit import AerBackend\n",
    "from qiskit.providers.aer.noise import NoiseModel\n",
    "from qiskit import IBMQ\n",
    "IBMQ.save_account('3c7bb882c186256cc5e989cf02f639e9282a264ee520c8882c6fb1c9ea51be3ce3d407629b8fed7450de430b7ccc59d4f97afddbba9b72b9daaebb40559ef0fc')\n",
    "provider = IBMQ.load_account()\n",
    "backend = AerBackend(NoiseModel.from_backend(IBMQ.providers()[0].get_backend('ibmq_manila')))\n",
    "\n",
    "backend_config = {\n",
    "    \"backend\": backend,\n",
    "    \"compilation\": backend.default_compilation_pass(2),\n",
    "    \"shots\": 2**10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46157d56-ad9c-4021-a23f-4c0a77b52e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTketModel.from_diagrams(np.array(all_circuits).reshape(-1), backend_config=backend_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835278b-325e-4766-a9d2-c4e348e35ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=np.finfo('float').eps\n",
    "loss = lambda y_hat, y: (-np.nansum(y * np.log(y_hat + epsilon)) + epsilon ) / len(y)\n",
    "acc = lambda y_hat, y: np.nansum(np.round(y_hat) == y) / len(y)/ 2 if (len(y) > 0) else None\n",
    "eval_metrics = {\"acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181371ad-eed4-4d90-aed8-872f395e37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = QuantumTrainer(\n",
    "    model,\n",
    "    loss_function=loss,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={\"a\": 0.05, \"c\": 0.06, \"A\": 0.01 * EPOCHS},\n",
    "    evaluate_functions=eval_metrics,\n",
    "    verbose=\"text\",\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d503ac9-1b22-41a7-b041-0187a6293eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Dataset(\n",
    "            train_circuits,\n",
    "            train_labels,\n",
    "            batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = Dataset(val_circuits, val_labels, shuffle=False)\n",
    "if not train_labels:\n",
    "    raise Exception(\"Empty train labels list!\")\n",
    "if not val_labels:\n",
    "    raise Exception(\"Empty test labels list!\")\n",
    "print(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885ad16-c79e-4f2e-98a4-153a3dd5fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "trainer.fit(train_dataset, val_dataset, logging_step=12)\n",
    "\n",
    "print(\"Training Time:--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e891f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('accuracy_vals_train_Sim15_N3_l2_noise2.npy', trainer.train_results['acc'])\n",
    "np.save('accuracy_vals_val_Sim15_N3_l2_noise2.npy', trainer.val_results['acc'])\n",
    "np.save('cost_vals_train_Sim15_N3_l2_noise2.npy', trainer.train_epoch_costs)\n",
    "np.save('cost_vals_val_Sim15_N3_l2_noise2.npy', trainer.val_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb3bda-7eff-48ed-8bb7-955336558703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1, ((ax_tl, ax_tr), (ax_bl, ax_br)) = plt.subplots(2, 2, sharey='row', figsize=(10, 6))\n",
    "\n",
    "ax_tl.set_title('Training set')\n",
    "ax_tr.set_title('Development set')\n",
    "ax_bl.set_xlabel('Epochs')\n",
    "ax_br.set_xlabel('Epochs')\n",
    "ax_bl.set_ylabel('Accuracy')\n",
    "ax_tl.set_ylabel('Loss')\n",
    "\n",
    "colours = iter(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "ax_tl.plot(trainer.train_epoch_costs, color=next(colours))\n",
    "\n",
    "ax_bl.plot(trainer.train_results['acc'], color=next(colours))\n",
    "\n",
    "ax_tr.plot(trainer.val_costs, color=next(colours))\n",
    "\n",
    "ax_br.plot(trainer.val_results['acc'], color=next(colours))\n",
    "\n",
    "\n",
    "# print test accuracy\n",
    "test_acc = acc(model(test_circuits), test_labels)\n",
    "print('Test accuracy:', test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a53607-039d-474c-90b9-2f601b503af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum.circuit import IQPansatz as IQP\n",
    "pprint = lambda c: print(str(c).replace(' >>', '\\n  >>'))\n",
    "pprint(IQP(2, [[0.1], [0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd8383-5b5c-4cca-a97f-9e7b5ac5896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Mapping\n",
    "from lambeq import CircuitAnsatz\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "from discopy.quantum.circuit import (Circuit, Discard, Functor, Id,\n",
    "                                     IQPansatz as IQP, qubit, Sim14ansatz)\n",
    "from discopy.quantum.gates import Bra, Ket, Rx, Rz\n",
    "from discopy.rigid import Box, Diagram, Ty\n",
    "\n",
    "from lambeq.ansatz import Symbol\n",
    "from lambeq.ansatz.circuit import _ArMapT\n",
    "\n",
    "\n",
    "class CustomAnsatz(CircuitAnsatz):\n",
    "    \"\"\"S YC Chen, et al's deep reinforcement learning ansatz.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 ob_map: Mapping[Ty, int],\n",
    "                 n_layers: int,\n",
    "                 n_single_qubit_params: int = 3,\n",
    "                 discard: bool = False,\n",
    "                 special_cases: Optional[Callable[[_ArMapT], _ArMapT]] = None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ob_map : dict\n",
    "            A mapping from :py:class:`discopy.rigid.Ty` to the number of\n",
    "            qubits it uses in a circuit.\n",
    "        n_layers : int\n",
    "            The number of IQP layers used by the ansatz.\n",
    "        n_single_qubit_params : int, default: 3\n",
    "            The number of single qubit rotations used by the ansatz.\n",
    "        discard : bool, default: False\n",
    "            Discard open wires instead of post-selecting.\n",
    "        special_cases : callable, optional\n",
    "            A function that transforms an arrow map into one specifying\n",
    "            special cases that should not be converted by the Ansatz\n",
    "            class.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(ob_map=ob_map, n_layers=n_layers,\n",
    "                         n_single_qubit_params=n_single_qubit_params)\n",
    "\n",
    "        if special_cases is None:\n",
    "            special_cases = self._special_cases\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_single_qubit_params = n_single_qubit_params\n",
    "        self.discard = discard\n",
    "        self.functor = Functor(ob=self.ob_map,\n",
    "                               ar=special_cases(self._ar))\n",
    "\n",
    "    def _ar(self, box: Box) -> Circuit:\n",
    "        label = self._summarise_box(box)\n",
    "        dom, cod = self._ob(box.dom), self._ob(box.cod)\n",
    "\n",
    "        n_qubits = max(dom, cod)\n",
    "        n_layers = self.n_layers\n",
    "        n_1qubit_params = self.n_single_qubit_params\n",
    "\n",
    "        if n_qubits == 0:\n",
    "            circuit = Id()\n",
    "        elif n_qubits == 1:\n",
    "            syms = [Symbol(f'{label}_{i}') for i in range(n_1qubit_params)]\n",
    "            rots = [Rx, Rz]\n",
    "            circuit = Id(qubit)\n",
    "            for i, sym in enumerate(syms):\n",
    "                circuit >>= rots[i % 2](sym)\n",
    "        else:\n",
    "            # We have three rotations for each qubit corresponding to\n",
    "            # the Euler decomposition\n",
    "            n_params = n_layers * n_qubits * 4\n",
    "            syms = [Symbol(f'{label}_{i}') for i in range(n_params)]\n",
    "            params: np.ndarray[Any, np.dtype[Any]] = np.array(syms).reshape(\n",
    "                    (n_layers, n_qubits, 3))\n",
    "            circuit = Sim14ansatz(n_qubits, params)\n",
    "\n",
    "        if cod > dom:\n",
    "            circuit <<= Id(dom) @ Ket(*[0]*(cod - dom))\n",
    "        elif self.discard:\n",
    "            circuit >>= Id(cod) @ Discard(dom - cod)\n",
    "        else:\n",
    "            circuit >>= Id(cod) @ Bra(*[0]*(dom - cod))\n",
    "        return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74887c-8a96-4bd9-b3b5-43562177512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, remove_cups\n",
    "params = [[0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9],\n",
    "          [0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9],\n",
    "         [0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9, 0.9]]\n",
    "sentence = \"John cooks delicious food\"\n",
    "parser = BobcatParser()\n",
    "diagram = parser.sentence2diagram(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95bd19-cdfd-4b18-92dc-c9236b831c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic types\n",
    "from lambeq import remove_cups, AtomicType\n",
    "import numpy as np\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "Sim14Ansatz = _sim_ansatz_factory(14)\n",
    "Sim15Ansatz = _sim_ansatz_factory(15)\n",
    "ansatz = Sim14Ansatz({N: 1, S: 1}, n_layers=1)\n",
    "discopy_Circ2 = ansatz(remove_cups(diagram))\n",
    "discopy_Circ2.draw(figsize=(20, 20), fontsize=12)\n",
    "discopy_circuit = Sim14ansatz(3, params)\n",
    "discopy_circuit.draw(figsize=(20, 20), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae61e62-01b2-4b8c-9f29-369cab512f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Callable, Mapping\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import os\n",
    "import random\n",
    "import socket\n",
    "import sys\n",
    "from typing import Any, Optional, Union\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "from discopy import Tensor\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lambeq.core.globals import VerbosityLevel\n",
    "from lambeq.training.checkpoint import Checkpoint\n",
    "from lambeq.training.dataset import Dataset\n",
    "from lambeq.training.model import Model\n",
    "\n",
    "\n",
    "def _import_tensorboard_writer() -> None:\n",
    "    global SummaryWriter\n",
    "    try:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "    except ImportError:  # pragma: no cover\n",
    "        raise ImportError('tensorboard not found. Please install it using '\n",
    "                          '`pip install tensorboard`.')\n",
    "\n",
    "\n",
    "_StrPathT = Union[str, 'os.PathLike[str]']\n",
    "\n",
    "\n",
    "class CustomQuantumTrainer(QuantumTrainer):\n",
    "\n",
    "\n",
    "    def validation_step(\n",
    "            self,\n",
    "            batch: tuple[list[Any], np.ndarray]) -> tuple[np.ndarray, float]:\n",
    "        \"\"\"Perform a validation step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : tuple of list and np.ndarray\n",
    "            Current batch.\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of np.ndarray and float\n",
    "            The model predictions and the calculated loss.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        assert len(y) > 0\n",
    "        #assert y_hat != 0\n",
    "        print('Size of y from validation function', len(y))\n",
    "        print('log(y_hat) from validation function', np.log(y_hat))\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "        numerator_loss = -np.sum(y * np.log(y_hat + epsilon)) + epsilon\n",
    "        denom_loss = len(y)\n",
    "        print('calculated numerator loss from validation function', numerator_loss)\n",
    "        print('calculated denom loss from validation function', denom_loss)\n",
    "        print('calculated loss from validation function', loss)\n",
    "        return y_hat, loss\n",
    "    def fit(self,\n",
    "            train_dataset: Dataset,\n",
    "            val_dataset: Optional[Dataset] = None,\n",
    "            evaluation_step: int = 1,\n",
    "            logging_step: int = 1) -> None:\n",
    "        \"\"\"Fit the model on the training data and, optionally,\n",
    "        evaluate it on the validation data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_dataset : :py:class:`Dataset`\n",
    "            Dataset used for training.\n",
    "        val_dataset : :py:class:`Dataset`, optional\n",
    "            Validation dataset.\n",
    "        evaluation_step : int, default: 1\n",
    "            Sets the intervals at which the metrics are evaluated on the\n",
    "            validation dataset.\n",
    "        logging_step : int, default: 1\n",
    "            Sets the intervals at which the training statistics are\n",
    "            printed if `verbose = 'text'` (otherwise ignored).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.from_checkpoint:\n",
    "            self._load_extra_chkpoint_info(self.checkpoint)\n",
    "\n",
    "        def writer_helper(*args: Any) -> None:\n",
    "            if self.use_tensorboard:\n",
    "                self.writer.add_scalar(*args)\n",
    "            else:\n",
    "                print(*args)\n",
    "\n",
    "        # initialise progress bar\n",
    "        step = self.start_step\n",
    "        batches_per_epoch = ceil(len(train_dataset)/train_dataset.batch_size)\n",
    "        status_bar = tqdm(total=float('inf'),\n",
    "                          bar_format='{desc}',\n",
    "                          desc=self._generate_stat_report(),\n",
    "                          disable=(\n",
    "                                self.verbose != VerbosityLevel.PROGRESS.value),\n",
    "                          leave=True,\n",
    "                          position=0)\n",
    "\n",
    "        # start training loop\n",
    "        for epoch in trange(self.start_epoch,\n",
    "                            self.epochs,\n",
    "                            desc='Epoch',\n",
    "                            disable=(\n",
    "                                self.verbose != VerbosityLevel.PROGRESS.value),\n",
    "                            leave=False,\n",
    "                            position=1):\n",
    "            train_loss = 0.0\n",
    "            with Tensor.backend(self.backend):\n",
    "                for batch in tqdm(train_dataset,\n",
    "                                  desc='Batch',\n",
    "                                  total=batches_per_epoch,\n",
    "                                  disable=(self.verbose\n",
    "                                           != VerbosityLevel.PROGRESS.value),\n",
    "                                  leave=False,\n",
    "                                  position=2):\n",
    "                    step += 1\n",
    "                    x, y_label = batch\n",
    "                    y_hat, loss = self.training_step(batch)\n",
    "                    writer_helper('y_hat', y_hat)\n",
    "                    #if np.isnan(np.array(y_hat)).any():\n",
    "                     #   raise Exception(\"Empty evaluation values list after training step!\")\n",
    "                    #if np.isnan(np.array(loss)).any():\n",
    "                     #   raise Exception(\"Nan loss values list after training step!\")\n",
    "                    writer_helper('y_hat', y_hat)\n",
    "                    if np.isnan(np.array(y_hat)).any():\n",
    "                        raise Exception(\"Empty evaluation values list!\")\n",
    "                    if (self.evaluate_on_train\n",
    "                            and self.evaluate_functions is not None):\n",
    "                        for metr, func in self.evaluate_functions.items():\n",
    "                            res = func(y_hat, y_label)\n",
    "                            metric = self._train_results_epoch[metr]\n",
    "                            metric.append(len(x) * res)\n",
    "                    train_loss += len(batch[0]) * loss\n",
    "                    assert np.isnan(np.array(loss)).any() == False\n",
    "                    writer_helper('train/step_loss', loss, step)\n",
    "                    status_bar.set_description(\n",
    "                            self._generate_stat_report(\n",
    "                                train_loss=loss,\n",
    "                                val_loss=(self.val_costs[-1] if self.val_costs\n",
    "                                          else None)))\n",
    "            train_loss /= len(train_dataset)\n",
    "            assert len(train_dataset) > 0\n",
    "            print('y_label', y_label)\n",
    "            if np.isnan(np.array(y_label)).any():\n",
    "                raise Exception(\"Empty train label list!\")\n",
    "            if np.isnan(np.array(y_hat)).any():\n",
    "                raise Exception(\"Empty evaluation values list!\")\n",
    "            self.train_epoch_costs.append(train_loss)\n",
    "            writer_helper('train/epoch_loss', train_loss, epoch + 1)\n",
    "\n",
    "            # evaluate on train\n",
    "            if (self.evaluate_on_train\n",
    "                    and self.evaluate_functions is not None):\n",
    "                for name in self._train_results_epoch:\n",
    "                    self.train_results[name].append(\n",
    "                        sum(self._train_results_epoch[name])/len(train_dataset)\n",
    "                    )\n",
    "                    self._train_results_epoch[name] = []  # reset\n",
    "                    writer_helper(\n",
    "                        f'train/{name}', self.train_results[name][-1],\n",
    "                        epoch+1)\n",
    "                    if self.verbose == VerbosityLevel.PROGRESS.value:\n",
    "                        status_bar.set_description(\n",
    "                                self._generate_stat_report(\n",
    "                                    train_loss=train_loss,\n",
    "                                    val_loss=(self.val_costs[-1]\n",
    "                                              if self.val_costs else None)))\n",
    "\n",
    "            # evaluate metrics on validation data\n",
    "            if val_dataset is not None:\n",
    "                if epoch % evaluation_step == 0:\n",
    "                    val_loss = 0.0\n",
    "                    batches_per_validation = ceil(len(val_dataset)\n",
    "                                                  / val_dataset.batch_size)\n",
    "                    writer_helper('batches_per_validation', batches_per_validation, len(val_dataset), val_dataset.batch_size)\n",
    "                    with Tensor.backend(self.backend):\n",
    "                        disable_tqdm = (self.verbose\n",
    "                                        != VerbosityLevel.PROGRESS.value)\n",
    "                        for v_batch in tqdm(val_dataset,\n",
    "                                            desc='Validation batch',\n",
    "                                            total=batches_per_validation,\n",
    "                                            disable=disable_tqdm,\n",
    "                                            leave=False,\n",
    "                                            position=2):\n",
    "                            writer_helper(\"***\", v_batch)\n",
    "                            x_val, y_label_val = v_batch\n",
    "                            writer_helper(\"***\", x_val, y_label_val)\n",
    "                            y_hat_val, cur_loss = self.validation_step(v_batch)\n",
    "                            if np.isnan(np.array(y_hat)).any():\n",
    "                                raise Exception(\"Empty evaluation values list after validation step!\")\n",
    "                            writer_helper(\"***\", y_hat_val, cur_loss)\n",
    "                            val_loss += cur_loss * len(x_val)\n",
    "                            if self.evaluate_functions is not None:\n",
    "                                for metr, func in (\n",
    "                                        self.evaluate_functions.items()):\n",
    "                                    res = func(y_hat_val, y_label_val)\n",
    "                                    self._val_results_epoch[metr].append(\n",
    "                                        len(x_val)*res)\n",
    "                            status_bar.set_description(\n",
    "                                    self._generate_stat_report(\n",
    "                                        train_loss=train_loss,\n",
    "                                        val_loss=val_loss))\n",
    "                        assert len(val_dataset) > 0\n",
    "                        writer_helper('empty val dataset!', len(val_dataset))\n",
    "                        val_loss /= len(val_dataset)\n",
    "                        self.val_costs.append(val_loss)\n",
    "                        writer_helper('cost vals', self.val_costs)\n",
    "                        status_bar.set_description(\n",
    "                                self._generate_stat_report(\n",
    "                                    train_loss=train_loss,\n",
    "                                    val_loss=val_loss))\n",
    "                        if np.isnan(np.array(val_loss)).any():\n",
    "                            raise Exception(\"loss value is NaN!\")\n",
    "                        writer_helper('empty val dataset!', len(val_dataset))\n",
    "                        writer_helper('val/loss', val_loss, epoch+1)\n",
    "\n",
    "                    if self.evaluate_functions is not None:\n",
    "                        for name in self._val_results_epoch:\n",
    "                            assert len(val_dataset) > 0\n",
    "                            writer_helper('empty val dataset!', len(val_dataset))\n",
    "                            self.val_results[name].append(\n",
    "                                sum(self._val_results_epoch[name])\n",
    "                                / len(val_dataset))\n",
    "                            self._val_results_epoch[name] = []  # reset\n",
    "                            writer_helper(\n",
    "                                f'val/{name}', self.val_results[name][-1],\n",
    "                                epoch + 1)\n",
    "                            status_bar.set_description(\n",
    "                                    self._generate_stat_report(\n",
    "                                        train_loss=train_loss,\n",
    "                                        val_loss=val_loss))\n",
    "            # save checkpoint info\n",
    "            save_dict = {'epoch': epoch+1,\n",
    "                         'model_weights': self.model.weights,\n",
    "                         'model_symbols': self.model.symbols,\n",
    "                        'train_costs': self.train_costs,\n",
    "                         'train_epoch_costs': self.train_epoch_costs,\n",
    "                         'train_results': self.train_results,\n",
    "                         'val_costs': self.val_costs,\n",
    "                         'val_results': self.val_results,\n",
    "                         'random_state': random.getstate(),\n",
    "                         'step': step}\n",
    "            print(f\"save_dict: {save_dict}\")\n",
    "            self.save_checkpoint(save_dict, self.log_dir)\n",
    "            if self.verbose == VerbosityLevel.TEXT.value:\n",
    "                if epoch == 0 or (epoch+1) % logging_step == 0:\n",
    "                    space = (len(str(self.epochs))-len(str(epoch+1)) + 2) * ' '\n",
    "                    prefix = f'Epoch {epoch+1}:' + space\n",
    "                    print(prefix + self._generate_stat_report(\n",
    "                            train_loss=train_loss,\n",
    "                            val_loss=(self.val_costs[-1] if self.val_costs\n",
    "                                      else None)),\n",
    "                          file=sys.stderr)\n",
    "        status_bar.close()\n",
    "        if self.verbose == VerbosityLevel.TEXT.value:\n",
    "            print('\\nTraining completed!', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776f9f5-6fc5-46cf-b8b2-c691d83d1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = CustomQuantumTrainer(\n",
    "    model,\n",
    "    epochs=EPOCHS,\n",
    "    loss_function=loss,\n",
    "    optimizer=SPSAOptimizer,\n",
    "    optim_hyperparams={'a': 0.05, 'c': 0.06, 'A':0.01*EPOCHS},\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    verbose = 'text',\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22523d-800e-4bb9-94b4-b1905683d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.fit(train_dataset, val_dataset, logging_step=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432561ec-ea32-433f-9843-c8fef39eb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from discopy.quantum import Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f584dda-bb59-460f-880b-0735b4f6896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_circuits).reshape(-1)\n",
    "all_circuits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2199504-eea9-4412-8d01-5b114cf94aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytket.circuit.display import render_circuit_jupyter\n",
    "render_circuit_jupyter(all_circuits[0][0].to_tk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a144b07-e07d-42cf-b0d3-5e44214316e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdified_diagram = model._get_lambda(train_circuits[0][0])\n",
    "print(len(val_circuits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77561f-de3a-4220-99be-827c22ff81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambdified_diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a9be0-2163-4df0-b480-626f9192991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0ee14-1d4a-49f7-bd67-1e90e5507189",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = Circuit.eval(\n",
    "    *lambdified_diagram(*model.weights),\n",
    "    **model.backend_config,\n",
    "    seed=model._randint(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e57019-af88-4658-b457-c7e14a1be2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdified_diagram(*model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfa50b-4805-4c59-a262-c6c92fb504fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epsilon=np.finfo('float64').eps\n",
    "eps = 1e-50\n",
    "np.log(eps + epsilon)/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155cdbc-17cf-4d7d-88a5-5c7cd9e3d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1a0c0-07bb-4aab-9350-52438cea3e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
